{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "### 가중합 WeightSum 계층 구현 : 맥락벡터를 구하는 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [],[]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):  # hs : (N,T,H)\n",
    "        N,T,H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N,T,1) # .repeat(H, axis=2)는 생략가능\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)  # (N,H)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self,dc) :\n",
    "        hs, ar = self.cache\n",
    "        N,T,H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N,1,H).repeat(T,axis=1)   # sum의 역전파, 출력: (N,T,H)\n",
    "        dar = dt * hs       # (N,T,H)\n",
    "        dhs = dt * ar       # (N,T,H)\n",
    "        da = np.sum(dar, axis=2)    # repeat의 역전파, 출력 :(N,T)\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_layers import softmax, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각단어의 가중치를 구하는 class\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [],[]\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h) :  # hs: (N,T,H),  h : (N,H)\n",
    "        N,T,H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N,1,H) # .repeat(T,axis=1)는 생략가능\n",
    "        t = hs * hr           #  (N,T,H)\n",
    "        s = np.sum(t, axis=2) #  (N,T)\n",
    "        a = self.softmax.forward(s)  # (N,T)\n",
    "        \n",
    "        self.cache = (hs,hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da) :\n",
    "        hs,hr = self.cache\n",
    "        N,T,H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)    # (N,T)\n",
    "        dt = ds.reshape(N,T,1).repeat(H,axis=2)  # (N,T,H)\n",
    "        dhs = dt * hr              # (N,T,H)\n",
    "        dhr = dt * hs              # (N,T,H)\n",
    "        dh = np.sum(dhr, axis=1)   # (N,H)\n",
    "        \n",
    "        return dhs, dh     # (N,T,H) , (N,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:  # AttentionWeight과 WeightSum으로 구성\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):  # hs_enc : Encoder의 출력값, hs_dec:하위 LSTM 출력값 이 입력된다\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight) # Attention 계층의 각 단어의 가중치를 리스트에 저장\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]     # Attention계층\n",
    "            dhs, dh = layer.backward(dout[:, t, :])  # dhs는 Encoder에 전달, dh는 LSTM계층으로 전달\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh        # h: LSTM 계층의 출력이므로\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_layers import Encoder, Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder): # Encoder class 상속받음\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs # 모든 hs 반환 (N,T,H)\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)  # dhs 그대로 사용\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()     # 추가된 부분\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]   # Encoder의 마지막줄\n",
    "        self.lstm.set_state(h) # TimeLSTM의 self.h 에 저장\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out) # TimeLSTM의 self.h와 함께 처리\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)  # TimeAttention계층의 출력과 LSTM계층의 출력을 합쳐서 Affine계층에 입력\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2   # Affine계층 입력시에 concatenate으로 합쳤기 때문에 반으로 나눈다, dec_hs:(N,T,H)\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]  # concatenate의 역전파\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1  # Affine의 역전파 출력과 Attention 역전파의 출력을 합하여 LSTM에 전달\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh  # Encoder의 마지막줄 + dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]   # Encoder의 마지막줄\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)  # LSTM출력\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(V, D, H)\n",
    "        self.decoder = AttentionDecoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 29) (45000, 11)\n",
      "(5000, 29) (5000, 11)\n",
      "[ 8 22  9 22  9  8  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7]\n",
      "[14 11 12  9  8 15 16  8 15 16  9]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from nn_layers import Softmax, Adam ,Trainer ,TimeEmbedding ,TimeSoftmaxWithLoss,TimeLSTM,TimeAffine\n",
    "from nn_layers import eval_seq2seq,Seq2seq ,PeekySeq2seq\n",
    "\n",
    "# date.txt : 날짜 형식 변환 데이터, 5만개의 날짜 변환 학습 데이터\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)\n",
    "print(x_train[0])\n",
    "print(t_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 13[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 26[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 38[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 50[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 63[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 76[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 88[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 100[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 112[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 124[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 136[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 148[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 160[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 172[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 184[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 197[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 208[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "정확도 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 12[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 23[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 35[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 48[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 61[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 74[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 86[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 98[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 110[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 121[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 133[s] | 손실 0.94\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 145[s] | 손실 0.90\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 158[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 171[s] | 손실 0.74\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 185[s] | 손실 0.66\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 198[s] | 손실 0.58\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 210[s] | 손실 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "정확도 51.640%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 12[s] | 손실 0.30\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 24[s] | 손실 0.21\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 36[s] | 손실 0.14\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 47[s] | 손실 0.09\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 59[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 70[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 82[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 93[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 103[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 114[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 125[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 136[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 147[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 158[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 169[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 180[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 190[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 11[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 22[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 33[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 44[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 55[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 66[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 77[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 100[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 111[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 123[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 134[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 146[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 157[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 169[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 180[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 191[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 46[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 91[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 103[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 126[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 137[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 149[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 160[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 171[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 182[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 194[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 91[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 113[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 125[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 136[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 147[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 159[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 170[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 181[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 193[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 113[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 146[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 158[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 169[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 180[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 192[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 56[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 101[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 147[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 159[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 170[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 182[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 193[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 35[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 46[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 58[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 69[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 92[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 103[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 115[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 126[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 138[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 150[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 161[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 173[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 184[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 196[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 46[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 69[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 92[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 103[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 126[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 137[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 149[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 160[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 172[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 183[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 195[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "# max_epoch = 1\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)  \n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 약 1시간 소요\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))   # 정확도 100.000%, Attention이 가장 유리하다\n",
    "\n",
    "\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
