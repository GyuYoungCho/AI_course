{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression : 2진 분류(Binary Classification)\n",
    "-  2진 분류의 활성화 함수로는 sigmoid가 사용됨\n",
    "\n",
    "#### sigmoid 함수\n",
    "- sigmoid : 2진 분류(Binary Classification,Logistic Regression) 모델의 활성화 함수(Activation Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "tf.random.set_seed(5)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1./(1. + math.e**-z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2],\n",
    "          [2,3],\n",
    "          [3,1],\n",
    "          [4,3],\n",
    "          [5,3],\n",
    "          [6,2]]\n",
    "\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "x_train = np.array(x_data,dtype=np.float32)\n",
    "y_train = np.array(y_data,dtype=np.float32)\n",
    "\n",
    "W = tf.Variable(tf.random.normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X):\n",
    "    return tf.sigmoid(tf.matmul(X,W) + b)\n",
    "\n",
    "def cost_func():\n",
    "    # cost = tf.reduce_mean(tf.square(hypothesis(x_train) - y_train)) # 회귀 모델\n",
    "\n",
    "    cost = -tf.reduce_mean(y_train*tf.math.log(hypothesis(x_train)) + \n",
    "                          (1 - y_train)*tf.math.log(1 - hypothesis(x_train)))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Start Learning!!\n",
      "0000 cost:[ 0.8277693 ]  W: [[ 1.3406279]\n",
      " [-0.1332199]]  b: [-1.1178697]\n",
      "1000 cost:[ 0.12722206 ]  W: [[1.5852704]\n",
      " [0.4354949]]  b: [-6.2488832]\n",
      "2000 cost:[ 0.05386779 ]  W: [[2.3047826 ]\n",
      " [0.98030955]]  b: [-10.019882]\n",
      "3000 cost:[ 0.027737541 ]  W: [[2.923696 ]\n",
      " [1.3474218]]  b: [-12.9787245]\n",
      "4000 cost:[ 0.015573931 ]  W: [[3.4833386]\n",
      " [1.650385 ]]  b: [-15.568397]\n",
      "5000 cost:[ 0.009132211 ]  W: [[4.009466 ]\n",
      " [1.9239582]]  b: [-17.968874]\n",
      "****** Learning Finished!!\n"
     ]
    }
   ],
   "source": [
    "print('****** Start Learning!!')\n",
    "for step in range(5001):\n",
    "    # cost를 minimize 한다\n",
    "    optimizer.minimize(cost_func,var_list=[W,b]) # W,b를 업데이트\n",
    "    if step % 1000 == 0:\n",
    "        print('%04d'%step,'cost:[',cost_func().numpy(),']',\n",
    "             ' W:',W.numpy(),' b:',b.numpy())\n",
    "        \n",
    "print('****** Learning Finished!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: [[4.009466 ]\n",
      " [1.9239582]]\n",
      "bias: [-17.968874]\n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "print('Weight:',W.numpy())\n",
    "print('bias:', b.numpy())\n",
    "\n",
    "def predict(X):\n",
    "    return tf.cast(hypothesis(X) > 0.5, dtype = tf.float32)\n",
    "\n",
    "x_test = x_train\n",
    "y_test = y_train\n",
    "\n",
    "preds = predict(x_test)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds,y_test),dtype=tf.float32))\n",
    "\n",
    "print('Accuracy : ',accuracy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-classification\n",
    ": multi-nomial classification (다중 분류) : Y값의 범주가 3개 이상인 분류\n",
    "#### 활성화 함수(Activation function) 으로 softmax함수 가 사용된다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data set :\n",
    "# x_data :  [N,4]  --> [8,4]\n",
    "x_data = [[1,2,1,1],\n",
    "          [2,1,3,2],\n",
    "          [3,1,3,4],\n",
    "          [4,1,5,5],\n",
    "          [1,7,5,5],\n",
    "          [1,2,5,6],\n",
    "          [1,6,6,6],\n",
    "          [1,7,7,7]]\n",
    "\n",
    "# y_data : [N,3] --> [8,3]\n",
    "y_data = [[0,0,1],  # [2]\n",
    "          [0,0,1],  # [2]\n",
    "          [0,0,1],  # [2]\n",
    "          [0,1,0],  # [1]\n",
    "          [0,1,0],  # [1]\n",
    "          [0,1,0],  # [1]\n",
    "          [1,0,0],  # [0]\n",
    "          [1,0,0]]  # [0]\n",
    "\n",
    "x_train = np.array(x_data,dtype=np.float32)\n",
    "y_train = np.array(y_data,dtype=np.float32)\n",
    "\n",
    "W = tf.Variable(tf.random.normal([4,3]), name = 'weight')\n",
    "b = tf.Variable(tf.random.normal([3]), name = 'bias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 비용 함수 구현 방법 1: log함수를 사용하여 수식을 직접 표현\n",
    "# def cost_func():\n",
    "#     cost = tf.reduce_mean(-tf.reduce_sum(y_train*tf.math.log(hypothesis(x_train)),\n",
    "#                                          axis=1))\n",
    "#     return cost\n",
    "# 비용함수 구현 방법 2 : tf.nn.softmax_cross_entropy_with_logits() 함수 사용\n",
    "def cost_func():\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits(x_train),\n",
    "                                             labels = y_train)\n",
    "    cost =  tf.reduce_mean(cost_i)\n",
    "    return cost\n",
    "\n",
    "def logits(X):\n",
    "    return tf.matmul(X,W) + b\n",
    "\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logits(X))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Start Learning!!\n",
      "0000 cost:[ 9.878908 ]  W: [[-0.70010203  0.36601627 -0.08132568]\n",
      " [-0.513514   -0.43817282  0.30525848]\n",
      " [ 0.12083036 -1.179445    1.4831539 ]\n",
      " [ 0.3534133  -1.5123805  -0.21457517]]  b: [-0.7218484  1.6085751 -1.087273 ]\n",
      "1000 cost:[ 0.36079222 ]  W: [[-2.2377305   0.2610561   1.4514554 ]\n",
      " [-0.00528285 -0.24690555 -0.0131535 ]\n",
      " [ 0.92016226  0.33322957 -0.08321315]\n",
      " [ 0.09760489  0.02930265 -1.3324299 ]]  b: [-3.2477362  -0.78599024  2.487776  ]\n",
      "2000 cost:[ 0.19642809 ]  W: [[-3.8626187   0.760477    2.2226121 ]\n",
      " [-0.14338577 -0.25901413  0.06708437]\n",
      " [ 2.3744597   0.46873924 -0.9336092 ]\n",
      " [-0.16464697  0.1332273  -1.2890192 ]]  b: [-6.547855  -1.5583214  4.9002404]\n",
      "3000 cost:[ 0.1104168 ]  W: [[-5.4746475   1.3234361   2.9324396 ]\n",
      " [-0.2666849  -0.25400275  0.12868723]\n",
      " [ 3.9054184   0.46343267 -1.7107009 ]\n",
      " [-0.56691575  0.2606404  -1.1930344 ]]  b: [-9.6826515 -2.0707116  7.0556283]\n",
      "4000 cost:[ 0.06373567 ]  W: [[-7.01363     1.883223    3.6       ]\n",
      " [-0.38968173 -0.24195696  0.1856712 ]\n",
      " [ 5.4364085   0.38649392 -2.4349256 ]\n",
      " [-1.0338299   0.41373473 -1.0861582 ]]  b: [-12.57387    -2.4770777   9.026617 ]\n",
      "5000 cost:[ 0.037346154 ]  W: [[-8.5016775   2.4364202   4.244631  ]\n",
      " [-0.5120406  -0.22722809  0.24062157]\n",
      " [ 6.9473467   0.27729416 -3.1290908 ]\n",
      " [-1.5285522   0.5747783  -0.9715662 ]]  b: [-15.291794   -2.8167117  10.876143 ]\n",
      "****** Learning Finished!!\n"
     ]
    }
   ],
   "source": [
    "print('****** Start Learning!!')\n",
    "for step in range(5001):\n",
    "    # cost를 minimize 한다\n",
    "    optimizer.minimize(cost_func,var_list=[W,b]) # W,b를 업데이트\n",
    "    if step % 1000 == 0:\n",
    "        print('%04d'%step,'cost:[',cost_func().numpy(),']',\n",
    "             ' W:',W.numpy(),' b:',b.numpy())\n",
    "        \n",
    "print('****** Learning Finished!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 1 1 1 0 0]\n",
      "[[3.8040382e-14 1.0293008e-05 9.9998975e-01]\n",
      " [7.5317377e-11 1.1370678e-02 9.8862934e-01]\n",
      " [6.9941707e-17 3.9894555e-02 9.6010542e-01]\n",
      " [2.2848299e-15 9.6675694e-01 3.3243079e-02]\n",
      " [7.0714615e-02 9.2695910e-01 2.3262901e-03]\n",
      " [3.7234403e-02 9.6271604e-01 4.9614675e-05]\n",
      " [9.0706098e-01 9.2937954e-02 1.0319754e-06]\n",
      " [9.9858642e-01 1.4136194e-03 1.7703172e-10]]\n",
      "[2 2 2 1 1 1 0 0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "# tf.argmax() : 값이 가장 큰 요소의 인덱스 값을 반환\n",
    "def predict(X):\n",
    "    return tf.argmax(hypothesis(X),axis=1)\n",
    "\n",
    "\n",
    "# 학습 데이터를 검증 데이터로 동일하게 사용하는 경우\n",
    "x_test = x_train\n",
    "y_test = y_train\n",
    "\n",
    "preds = predict(x_test)\n",
    "print(preds.numpy())\n",
    "print(hypothesis(x_test).numpy())\n",
    "print(tf.argmax(y_test,1).numpy())\n",
    "\n",
    "correct_predict = tf.equal(predict(x_test),tf.argmax(y_test,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, dtype = tf.float32))\n",
    "print(\"Accuracy:\",accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
